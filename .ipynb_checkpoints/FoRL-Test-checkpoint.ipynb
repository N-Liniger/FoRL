{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_examples\n",
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('gym_examples/GridWorld-v3', render_mode=\"human\", size=3, rewards=np.array([[1,2,3],[4,5,6],[7,8,9]]), costs=np.array([[1,2,3],[4,5,6],[7,8,9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adro\\Anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:252: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': array([1, 2])} [6, 6] False False {}\n",
      "{'agent': array([1, 1])} [5, 5] False False {}\n",
      "{'agent': array([1, 2])} [6, 6] False False {}\n",
      "{'agent': array([1, 1])} [5, 5] False False {}\n",
      "{'agent': array([1, 0])} [4, 4] False False {}\n",
      "{'agent': array([0, 0])} [1, 1] False False {}\n",
      "{'agent': array([0, 0])} [1, 1] False False {}\n",
      "{'agent': array([1, 0])} [4, 4] False False {}\n",
      "{'agent': array([0, 0])} [1, 1] False False {}\n",
      "{'agent': array([1, 0])} [4, 4] False False {}\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward_cost, terminated, truncated, info = env.step(action)\n",
    "    print(observation, reward_cost, terminated, truncated, info)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy is a map from the states to a distribution over actions\n",
    "# We assume the same actions can be taken from each state and they are\n",
    "# indexed by integers starting at 0\n",
    "class Policy:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        # Initialise as uniform distribution\n",
    "        self.S = num_states\n",
    "        self.A = num_actions\n",
    "        self.distribution = np.ones((num_states, num_actions))/num_actions\n",
    "    \n",
    "    # sample an action according to the policy in the given state \n",
    "    def __call__(self, state):\n",
    "        return np.random.choice(np.arange(self.A), p=self.distribution[state])\n",
    "    \n",
    "class SoftmaxPolicy(Policy):\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super().__init__(num_states, num_actions)\n",
    "        self.params = np.ones((self.S, self.A))\n",
    "        \n",
    "        \n",
    "    def __call__(self, state):\n",
    "        p = np.exp(self.params[state]) / np.sum(np.exp(self.params[state]))\n",
    "        return np.random.choice(np.arange(self.A), p = p)\n",
    "        \n",
    "    \n",
    "# Hazan paper between equations 2.4 and 2.5, mixture policy isnt what you thought it was\n",
    "class MixturePolicy():\n",
    "    def __init__(self, alpha, C):\n",
    "        self.alpha = alpha\n",
    "        self.C = C\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "    alpha: np.array of weights\n",
    "    C: list of policies\n",
    "\"\"\"\n",
    "def generateMixturePolicy(alpha, C):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = SoftmaxPolicy(12,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    policy: policy to compute the state distribution of\n",
    "    eps0: error tolerance\n",
    "\"\"\"\n",
    "def DensityOracle(policy, eps0):\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    eps1: error tolerance (should there be two eps here for reward\n",
    "    and constraint violations?)\n",
    "\"\"\"\n",
    "def MinMaxOracle(, eps1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ent_R = lambda d: -np.sum(d * np.log(d))\n",
    "max_ent_dR = lambda d: -(np.log(d)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0.3, 0.4, 0.2, 0.1])\n",
    "b = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "c = np.array([0.9, 0.05, 0.03, 0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38629436, 0.38629436, 0.38629436, 0.38629436])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_ent_dR(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM 1\n",
    "\"\"\"\n",
    "    env: the CMDP environment \n",
    "    R: the (convex) objective in the state distribution\n",
    "    dR: gradient of the (convex) objective for ease of computation\n",
    "    eta: step size\n",
    "    T: number of iterations\n",
    "    K: the constraint limit (should this be here on in the environment?)\n",
    "    eps0: state distribution oracle error tolerance\n",
    "    eps1: MinMax Oracle error tolerance\n",
    "\"\"\"\n",
    "def algorithm1(env, R, dR, eta, T, K, eps0, eps1):\n",
    "    C = [Policy()] # arbitrary policy\n",
    "    alpha = np.array([1])\n",
    "    for t in range(T):\n",
    "        \n",
    "        mixture_policy = Policy(alpha, C)\n",
    "        d = DensityOracle(mixture_policy, eps0)\n",
    "        \n",
    "        new_policy = MinMaxOracle(dR(d), K, eps1)\n",
    "        \n",
    "        C.append(new_policy)\n",
    "        alpha = np.append((1-eta)*alpha, eta)\n",
    "        \n",
    "    return Policy(alpha, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3bc25d0cb4b8219b27234986137c88689bcde7dbbcaab25aad9634a3ce866110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
