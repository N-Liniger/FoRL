{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT:\n",
    "- Clip params in softmax parameterization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_examples\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.size'] = 12\n",
    "rcParams['figure.figsize'] = (10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action2str(action):\n",
    "    left = u'\\u2190'\n",
    "    right = u'\\u2192'\n",
    "    up = u'\\u2191'\n",
    "    down = u'\\u2193'\n",
    "    policy_str = \"\"\n",
    "    if 0 == action:\n",
    "        policy_str += down \n",
    "    if 1 == action:\n",
    "        policy_str += right \n",
    "    if 2 == action:\n",
    "        policy_str += up\n",
    "    if 3 == action:\n",
    "        policy_str += left\n",
    "    return policy_str\n",
    "\n",
    "def getAngle(arr):\n",
    "    down, right, up, left = arr\n",
    "    x = right-left\n",
    "    y = up-down\n",
    "    if x > 0 and y >= 0: return np.arctan(y/x) /np.pi*180\n",
    "    if x < 0 and y >= 0: return 180 - np.arctan(y/abs(x)) /np.pi*180\n",
    "    if x > 0 and y < 0: return 360 - np.arctan(abs(y)/x) /np.pi*180\n",
    "    if x < 0 and y < 0: return 180 + np.arctan(abs(y)/abs(x)) /np.pi*180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy is a map from the states to a distribution over actions\n",
    "# We assume the same actions can be taken from each state and they are\n",
    "# indexed by integers starting at 0\n",
    "\"\"\"\n",
    "    size: size of the grid of the environment\n",
    "    A: number of actions\n",
    "    S: number of states (not currently used)\n",
    "\"\"\"\n",
    "class Policy:\n",
    "    def __init__(self, size, A=0, S=0):\n",
    "        # Initialise as uniform distribution\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        self.size = 3\n",
    "        self.distribution = np.ones((3, 3, A))/A\n",
    "    \n",
    "    # sample an action according to the policy in the given state \n",
    "    def __call__(self, state):\n",
    "        return np.random.choice(np.arange(self.A), p=self.distribution[state])\n",
    "    \n",
    "    def plotPolicy(self):\n",
    "        #size = int(np.sqrt(self.S))\n",
    "        plt.imshow(np.zeros((self.size, self.size)))\n",
    "        #actions = np.argmax(self.distribution, axis=1)\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                angle = getAngle(self.distribution[i,j])\n",
    "                plt.text(j, i, u'\\u2192', ha=\"center\", va=\"center\", color=\"r\", fontsize=24,\n",
    "                        rotation=angle, rotation_mode='anchor')\n",
    "    \n",
    "class SoftmaxPolicy(Policy):\n",
    "    def __init__(self, size, A, S=0):\n",
    "        super().__init__(size, A, S)\n",
    "        self.params = np.ones((self.size, self.size, self.A))\n",
    "        \n",
    "        \n",
    "    def __call__(self, state):\n",
    "        p = np.exp(self.params[state]) / np.sum(np.exp(self.params[state]))\n",
    "        return np.random.choice(np.arange(self.A), p = p)\n",
    "    \n",
    "    def plotPolicy(self):\n",
    "        self.updateDistribution()\n",
    "        plt.imshow(np.zeros((self.size, self.size)))\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                angle = getAngle(self.distribution[i,j])\n",
    "                plt.text(j, i, u'\\u2192', ha=\"center\", va=\"center\", color=\"r\", fontsize=24,\n",
    "                        rotation=angle, rotation_mode='anchor')\n",
    "    \n",
    "    def normaliseParams(self):\n",
    "        self.params = self.params - np.repeat(np.reshape(np.mean(self.params, axis=2), (self.size, self.size, 1)), \n",
    "                                              self.A, axis=2)\n",
    "    \n",
    "    def updateDistribution(self):\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                self.distribution[i,j] = np.exp(self.params[i,j]) / np.sum(np.exp(self.params[i,j]))\n",
    "        \n",
    "    \n",
    "# Have to \"select\" a policy at the start of each episode\n",
    "class MixturePolicy():\n",
    "    def __init__(self, alpha, C):\n",
    "        self.alpha = alpha\n",
    "        self.C = C\n",
    "        self.activePolicy = None\n",
    "        \n",
    "    def selectPolicy(self):\n",
    "        self.activePolicy = np.random.choice(self.C, p=self.alpha)   \n",
    "        \n",
    "    def __call__(self, state):\n",
    "        return self.activePolicy(state)\n",
    "    \n",
    "    def plotPolicy(self):\n",
    "        for pol in self.C:\n",
    "            pol.plotPolicy()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    env: the CMDP environment\n",
    "    policy: mixture policy to compute the state distribution of\n",
    "    eps0: error tolerance\n",
    "    M: number of episodes\n",
    "    t0: number of time steps per episode\n",
    "\"\"\"\n",
    "def DensityOracle(env, mix_policy, eps, eps0, M=10, t0=1000):\n",
    "    \n",
    "    #env = gym.make('gym_examples/GridWorld-v3', render_mode=None, size=3,\n",
    "    #               rewards=np.array([[1,2,3],[4,5,6],[7,8,9]]), costs=np.array([[1,2,3],[4,5,6],[7,8,9]]))\n",
    "    \n",
    "    delta = 0.05\n",
    "    gamma = 0.9\n",
    "    #M = int(200/(eps0**2) * np.log(2*S*np.log(0.1*eps)/(delta*np.log(gamma))))\n",
    "    #t0 = int(np.log(0.1*eps0)/np.log(gamma))\n",
    "    print(M, t0)\n",
    "    \n",
    "    state_frequencies = np.zeros((M, env.size, env.size))\n",
    "    \n",
    "    \n",
    "    for m in range(M):\n",
    "        observation = env.reset()[0]['agent']\n",
    "        state = (observation[0], observation[1])\n",
    "        cur_gamma = 1\n",
    "        mix_policy.selectPolicy()\n",
    "        for t in range(t0):\n",
    "            \n",
    "            action = mix_policy(state)  # agent policy that uses the observation and info\n",
    "            obs = env.step(action)[0]['agent']\n",
    "            state_frequencies[m, obs[0], obs[1]] += cur_gamma\n",
    "            cur_gamma *= gamma\n",
    "            \n",
    "\n",
    "    env.close()\n",
    "    return np.sum(state_frequencies, axis=0)/M*(1-gamma)/(1-np.exp(np.log(gamma)*t0))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    env: the CMDP environment\n",
    "    reward: vector of rewards\n",
    "    policy: policy\n",
    "    eps1: error tolerance (should there be two eps here for reward\n",
    "    and constraint violations?)\n",
    "    T: number of iterations\n",
    "    K: number of episodes per iteration\n",
    "    b: utility threshold\n",
    "\"\"\"\n",
    "def MinMaxOracle(env, policy, reward, eps1, T=10, K=10,  b=5, lr_factor=1, lamb_boost = 10):\n",
    "    \n",
    "    # Normalising the reward (could also normalise with max)\n",
    "    rewards = reward/np.sum(reward)\n",
    "    print('Rewards: \\n', rewards)\n",
    "    env = gym.make('gym_examples/GridWorld-v3', render_mode=None, size=env.size,\n",
    "                  rewards=rewards, costs=env.costs)\n",
    "    \n",
    "    # Initialise policy as uniform\n",
    "    policy = SoftmaxPolicy(env.size, env.A) # <====\n",
    "    \n",
    "    gamma = 0.9 # <====\n",
    "    \n",
    "    # Chosen according to theorem 1\n",
    "    eta1 = 2*np.log(env.A)/lr_factor\n",
    "    eta2 = (1-gamma)/np.sqrt(T)/lr_factor*lamb_boost\n",
    "    xi = 1/8 # <====\n",
    "    \n",
    "    lamb = 0; \n",
    "    for t in range(T):\n",
    "        VL = np.zeros((env.size, env.size)); QL = np.zeros((env.size, env.size, env.A))\n",
    "        AL = np.zeros((env.size, env.size, env.A))\n",
    "        \n",
    "        for k in range(K):\n",
    "            # Sampling (s,a) from initial state-action space (is action_space.sample() fine here? Different actions in different\n",
    "            # states may prove problematic)\n",
    "            \n",
    "            K_ = np.random.geometric(1-gamma) # Use the same or different K_ for Q_hat and V_hat? # <====\n",
    "            for i in range(env.size):\n",
    "                for j in range(env.size):\n",
    "                    # Setting the environment to the correct state (Is this a restrictive assumption??) # <========\n",
    "                    env.reset(state=np.array([i,j]))\n",
    "\n",
    "                    # Estimating V_hat\n",
    "                    state = (i,j)\n",
    "                    for k_ in range(K_): \n",
    "                        action = policy(state)\n",
    "                        observation, reward_cost, terminated, truncated, info = env.step(action)\n",
    "                        state = (observation['agent'][0], observation['agent'][1])\n",
    "                        \n",
    "                        VL[i,j] += reward_cost[0] + lamb*reward_cost[1]\n",
    "\n",
    "\n",
    "                    for a in range(env.A):\n",
    "                        # Estimating Q_hat\n",
    "                        env.reset(state=np.array([i,j]))\n",
    "                        action = a\n",
    "                        for k_ in range(K_):\n",
    "                            observation, reward_cost, terminated, truncated, info = env.step(action)\n",
    "                            state = (observation['agent'][0], observation['agent'][1])\n",
    "                            action = policy(state)\n",
    "\n",
    "                            QL[i,j,a] += reward_cost[0] + lamb*reward_cost[1]\n",
    "\n",
    "            Vg_hat_rho = 0\n",
    "            #for k in range(K):\n",
    "            observation, info = env.reset()\n",
    "            state = (observation['agent'][0], observation['agent'][1])\n",
    "            #K_ = np.random.geometric(1-gamma)\n",
    "            #Vg_hat_s = 0\n",
    "            for k_ in range(K_): \n",
    "                action = policy(state)\n",
    "                observation, reward_cost, terminated, truncated, info = env.step(action)\n",
    "                state = (observation['agent'][0], observation['agent'][1])\n",
    "                #Vg_hat_s += reward_cost[1]\n",
    "                Vg_hat_rho += reward_cost[1]\n",
    "            #Vg_hat_rho += Vg_hat_s/K\n",
    "        Vg_hat_rho = Vg_hat_rho / K\n",
    "            \n",
    "        # This should be the same as the way they do it\n",
    "        VL = VL / K # <====\n",
    "        QL = QL / K\n",
    "             \n",
    "        AL = QL - np.repeat(np.reshape(VL, (env.size, env.size,1)), env.A, axis=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        policy.params += eta1/(1-gamma)*AL\n",
    "        \n",
    "        \n",
    "        policy.normaliseParams()\n",
    "        #policy.updateDistribution()\n",
    "        #print(policy.distribution)\n",
    "        \n",
    "        #policy.plotPolicy()\n",
    "        #plt.show()\n",
    "        #min(1/((1-gamma)*xi), lamb - eta2*(Vg_hat_rho - b))\n",
    "        lamb = max(0, min(1/((1-gamma)*xi), lamb - eta2*(Vg_hat_rho - b)))\n",
    "        #print(Vg_hat_rho, lamb - eta2*(Vg_hat_rho - b), lamb)\n",
    "        #print(Vg_hat_rho)\n",
    "        #print(lamb)\n",
    "        \n",
    "    policy.plotPolicy()\n",
    "    plt.show()\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM 1\n",
    "\"\"\"\n",
    "    env: the CMDP environment \n",
    "    R: the (convex) objective in the state distribution\n",
    "    dR: gradient of the (convex) objective for ease of computation\n",
    "    eta: step size\n",
    "    T: number of iterations\n",
    "    K: the constraint limit (should this be here on in the environment?)\n",
    "    eps0: state distribution oracle error tolerance\n",
    "    eps1: MinMax Oracle error tolerance\n",
    "\"\"\"\n",
    "def algorithm1(env, dR, eps, eta, eps0, eps1, b=0, T=10, Tmm=20, K=20):\n",
    "    init_policy = SoftmaxPolicy(env.size, env.A)\n",
    "    init_policy.params = np.array([\n",
    "                          [[0.9, 0.05, 0.03, 0.02],\n",
    "                           [0.05, 0.9, 0.03, 0.02],\n",
    "                           [0.05, 0.03, 0.9, 0.02]],\n",
    "        \n",
    "                          [[0.02, 0.05, 0.03, 0.9],\n",
    "                           [0.5, 0.45, 0.025, 0.025],\n",
    "                           [0.5, 0.45, 0.025, 0.025]],\n",
    "        \n",
    "                          [[0.5, 0.45, 0.025, 0.025],\n",
    "                           [0.5, 0.45, 0.025, 0.025],\n",
    "                           [0.5, 0.45, 0.025, 0.025]]\n",
    "                        ])\n",
    "    init_policy.updateDistribution()\n",
    "    init_policy.plotPolicy()\n",
    "    plt.show()\n",
    "    C = [init_policy] # arbitrary policy\n",
    "    alpha = np.array([1])\n",
    "    for t in range(T):\n",
    "    \n",
    "        mixture_policy = MixturePolicy(alpha, C)\n",
    "        d = DensityOracle(env, mixture_policy, eps, eps0, M=1000, t0=100)\n",
    "        new_policy = MinMaxOracle(env, 0, dR(d),  eps1, T=Tmm, K=K,  b=b, lr_factor=1)\n",
    "        \n",
    "        C.append(new_policy)\n",
    "        alpha = np.append((1-eta)*alpha, eta)\n",
    "        \n",
    "    return MixturePolicy(alpha, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_examples/GridWorld-v3', render_mode=None, size=3, \n",
    "               rewards=np.array([[1,2,3],[4,5,6],[7,8,9]]), costs=np.array([[1,1,1],[1,1,1],[1,1,1]])/9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Oracle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.array([[1,2,1],[1,5,1],[1,3,1]])\n",
    "policy = SoftmaxPolicy(3, 4)\n",
    "policy = MinMaxOracle(env, policy, np.array([[1,2,1],\n",
    "                                                 [1,5,1],\n",
    "                                                 [1,3,1]]), 0, T=20, K=20, b=1/4, lr_factor=1, lamb_boost=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = policy\n",
    "alpha = [1]\n",
    "C = [policy]\n",
    "mix_p = MixturePolicy(alpha, C)\n",
    "res = DensityOracle(env, mix_p, 0,0.1, M=50, t0=100)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=5\n",
    "sigma = 0.1*eps / (2*env.S)\n",
    "eps0 = 0.1*eps**2/(80*env.S)\n",
    "eps1 = 0.1*eps\n",
    "eta = 0.1*eps**2/(40*env.S)\n",
    "T= 40*env.S*np.log(np.log(env.S)/(0.1*eps))/(0.1*eps**2)\n",
    "\n",
    "smoothed_max_ent = lambda d: np.sum(-np.log(d+sigma))\n",
    "smoothed_max_ent_dR = lambda d: -(d/(d+sigma)+np.log(d+sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "delta = 0.05\n",
    "M = int(200/(eps0**2) * np.log(2*env.S*np.log(0.1*eps)/(delta*np.log(gamma))))\n",
    "t0 = int(np.log(0.1*eps0)/np.log(gamma))\n",
    "M, t0, sigma, eps0, eps1, eta, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_policy = algorithm1(env, smoothed_max_ent_dR, eps=eps, eta=1e-2, eps0=eps0, eps1=eps1, T=200, Tmm=20, K=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = DensityOracle(env, mix_policy, 0, 0.1, M=5000, t0=100)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Policy(9, 4)\n",
    "p.distribution = np.array([[0.9, 0.05, 0.03, 0.02],\n",
    "                           [0.05, 0.9, 0.03, 0.02],\n",
    "                           [0.05, 0.03, 0.9, 0.02],\n",
    "                           [0.02, 0.05, 0.03, 0.9],\n",
    "                           [0.5, 0.45, 0.025, 0.025],\n",
    "                           [0.5, 0.45, 0.025, 0.025],\n",
    "                           [0.5, 0.45, 0.025, 0.025],\n",
    "                           [0.5, 0.45, 0.025, 0.025],\n",
    "                           [0.5, 0.45, 0.025, 0.025]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_examples/GridWorld-v3', render_mode=None, size=3, rewards=np.array([[1,2,3],[4,5,6],[7,8,9]]), costs=np.array([[1,2,3],[4,5,6],[7,8,9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward_cost, terminated, truncated, info = env.step(action)\n",
    "    print(observation, reward_cost, terminated, truncated, info)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_ent_R = lambda d: -np.sum(d * np.log(d))\n",
    "max_ent_dR = lambda d: -(np.log(d)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('gym_examples/GridWorld-v3', render_mode=None, size=3,\n",
    "#                  rewards=np.array([[1,2,3],[4,5,6],[7,8,9]])/45, costs=np.array([[1,1,1],[1,1,1],[1,1,1]])/9)\n",
    "env = gym.make('gym_examples/GridWorld-v3', render_mode=None, size=3,\n",
    "                  rewards=np.array([[1,2,3],[4,5,6],[7,8,9]])/49, costs=np.array([[1,0,0],[0,0,0],[0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = policy\n",
    "p.updateDistribution()\n",
    "p.distribution, p.params, np.argmax(p.distribution, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3bc25d0cb4b8219b27234986137c88689bcde7dbbcaab25aad9634a3ce866110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
